### 爬虫流程

```flow
st=>start: 开始
e=>end: 结束
op=>operation: url list
op1=>operation: 响应内容
op2=>operation: 提取数据
op3=>operation: 数据入库
cond=>condition: 提取url？

st->op->op1->cond
cond(no)->op
cond(yes)->op2->op3

```
##### http和https
    http 端口80
    https 端口443 是http+ssl安全套接字
    s 非对称加密

#### Requests模块的基本使用
`response = request.get(url)`

通过request发送请求 获取响应

* 获取源码的方式

    1.`response.content.decode()`
       `response.content.decode("GBK")`
    2.`response.encoding=""utf-8`
    `response.text`

* requests保存图片
    
    ```python
    response = requests.get(url="http://www.cdhfyllh.com/uploads/20140228/201402281322501136.jpg")
    with open("1.jpg","wb") as f:
        f.write(response.content)
    
    ```
    
* 发送header请求
    
```python

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
requests.get(url,headers=headers)

```
* 发送带参数的请求
    
```python
    
    # https://www.baidu.com/s?wd=python&c=b
    kw = {'wd':'长城'}
    requests.get(url,params=kw)
```
    **注意**： _在url地址中，很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除对应的，在后续的爬虫中，越到很多参数的url地址，都可以尝试删除参数_

* 文件写入
    `'w+' == w+r（可读可写，文件若不存在就创建）`

    `'a+' ==a+r（可追加可写，文件若不存在就创建）`
    
#### requests 深入使用
* requests发送post请求
    
    `response = requests.post("http://www.baidu.com/", data = data,headers=headers)`
**data是字典**

* 使用代理
`* proxies = { 
    "http": "http://12.34.56.79:9527", 
    "https": "https://12.34.56.79:9527", 
    }`

* requests post 发送 json数据

```python
import requests
import json

url = "http://39.107.229.4/found"

data = {"keyword":"rrr"}


headers = {
    "Content-Type":"application/json"
}

resp = requests.post(url=url,data=json.dumps(data),headers=headers)

print(resp.content.decode())

# 要区分dump方法与dumps方法
```

#### requets模块处理cookie和session

requests 提供了一个叫做session类，来实现客户端和服务端的会话保持

* 会话保持有两个内涵：
    * 保存cookie
    * 实现和服务端的长连接
             
            session = requests.session()
            response = session.get(url,headers)

    session实例在请求了一个网站后，对方服务器设置在本地的cookie会保存在session中，下一次再使用session请求对方服务器的时候，会带上前一次的cookie

#### requests处理证书错误
返回`ssl.CertificateError ...`

```
import requests

url = "https://www.12306.cn/mormhweb/"
response = requests.get(url,verify=False)
```

#### 超时参数的使用
`response = requests.get(url,timeout=3)`

#### retrying模块的使用

```python
import requests
from retrying import retry

headers = {}


@retry(stop_max_attempt_number=3) #最大重试3次，3次全部报错，才会报错
def _parse_url(url)
    response = requests.get(url, headers=headers, timeout=3) #超时的时候回报错并重试
    assert response.status_code == 200 #状态码不是200，也会报错并充实
    return response


def parse_url(url)
    try: #进行异常捕获
        response = _parse_url(url)
    except Exception as e:
        print(e)
        response = None
    return response
```


